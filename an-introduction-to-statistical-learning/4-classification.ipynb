{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.sm_exceptions import HessianInversionWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore', HessianInversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def compute_model(y_name, X_name, data):\n",
    "    y = data.loc[:, y_name]\n",
    "    X = sm.add_constant(data.loc[:, X_name].values)\n",
    "    model = sm.Logit(y, X).fit(disp=0)\n",
    "    \n",
    "    return show_table(model, X_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_table(model, X_name):\n",
    "    index_name = ['Intercept']\n",
    "    if isinstance(X_name, str):\n",
    "        index_name.append(X_name)\n",
    "    elif isinstance(X_name, list):\n",
    "        index_name = index_name + X_name\n",
    "    \n",
    "    df = pd.read_html(model.summary2().as_html())[1]\n",
    "    colname = df.iloc[0]\n",
    "    df = df.rename(columns=df.iloc[0]).drop(0).set_index(np.nan)\n",
    "    df.index.name = None\n",
    "    df.index = index_name\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = pd.read_csv('data/Default.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 4.1\n",
    "default_dummy = pd.get_dummies(default, columns=['default'])\n",
    "compute_model('default_Yes', 'balance', default_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 4.2\n",
    "default_dummy = pd.get_dummies(default, columns=['default', 'student'])\n",
    "compute_model('default_Yes', 'student_Yes', default_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 4.3\n",
    "default_dummy = pd.get_dummies(default, columns=['default', 'student'])\n",
    "compute_model('default_Yes', ['balance', 'income', 'student_Yes'], default_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border:1px solid black; padding:10px'>\n",
    "\n",
    "**Confounder** (also **confounding variable**, **confounding factor**, or **lurking variable**): a variable that influences both the dependent variable and independent variable, causing a spurious association.\n",
    "\n",
    "**Spurious relationship** (or **spurious correlation**): a mathematical relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor.\n",
    "\n",
    "**Example**: Two events can cause grass to be wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkle (when it rains, the sprinkler is usually not active).\n",
    "> <img src='images/bayesian_network.png' width='430px'>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear discriminant analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression involves directly modeling $\\Pr{(Y=k | X=x)}$ using the logistic function. \n",
    "\n",
    "Consider an alternative approach where we model the distribution of $X$ separately in each of the response classes, and then use Bayes' theorem to flip these around into estimates for $\\Pr{(Y=k | X=x)}$.\n",
    "\n",
    "Why do we need another method?\n",
    "\n",
    "* When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. LDA is not.\n",
    "* If $n$ is small and the distribution of the predictors $X$ is approximately normal in each of the classes, LDA is more statble than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bayes' theorem for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left:** Two normal density function $f_1(x)$ (with $\\mu_1$ = -1.25 and $\\sigma_1^2$ = 1) and $f_2(x)$ (with $\\mu_2$ = -1.25 and $\\sigma_2^2$ = 1)\n",
    "\n",
    "<div style='float:right; border:1px solid black; padding:10px; padding-left:15px; width:260px'>\n",
    "\n",
    "**Posterior probability**: the probability an event will happen after all evidence or background information has been taken into account. It is closely related to prior probability, which is the probability an event will happen before you taken any new evidence into account.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style='margin-right:280px'>\n",
    "Suppose that we classify an observation into one of $K$ classes ($K \\geq 2$).\n",
    "\n",
    "* Let $p_k(X) = \\Pr{(Y=k | X)}$ be the posterior probability that an observation $X = x$ belongs to the $k$th class.\n",
    "* Let $\\pi_k$ represent the overall or prior probability that a randomly chosen observation is associated with the $k$th category of the response variable $Y$.\n",
    "* Let $f_k(x) \\equiv \\Pr{(X=x | Y=k)}$ denote the density function of $X$ for an observation that comes from the $k$th class.\n",
    "    * $f_k(x)$ large $\\Rightarrow$ very likely that an observation in the $k$th class has $X \\approx x$.\n",
    "    * $f_k(x)$ small $\\Rightarrow$ very unlikely that an observation in the $k$th class has $X \\approx x$.\n",
    "</div>\n",
    "\n",
    "**Bayes' theorem**: \n",
    "$\\Pr{(Y=k | X=x)} = \\dfrac{\\pi_k f_k(x)}{\\sum\\limits_{l=1}^K{\\pi_l f_l(x)}}$\n",
    "\n",
    "* This equation suggests that instead of directly computing $p_k(X)$, we can plug in estimates for $\\pi_k$ and $f_k(X)$.\n",
    "    * Estimating $\\pi_k$ is easy if we have a random sample of $Y$s from the population.\n",
    "        * Compute the fraction of the training observations that belong to the $k$th class.\n",
    "    * Estimating $f_k(X)$ is more challenging, unless we assume some simple forms for these densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA for $p=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we only have one predictor. Hence, $p=1$. We want to obtain an estimate for $f_k(x)$ to plug into Bayes' theorem to estimate $p_k(x)$.\n",
    "\n",
    "Suppose we assume that $f_k(x)$ is *normal* or *Gaussian*.\n",
    "\n",
    "Normal density: $f_k(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp{\\left(-\\dfrac{1}{2\\sigma_k^2}(x - \\mu_k)^2\\right)}$\n",
    "    \n",
    "&emsp;where $\\mu_k$ and $\\sigma_k^2$ are the mean and variance parameters for the $k$th class.\n",
    "    \n",
    "Assume that $\\sigma_1^2 = ... = \\sigma_K^2$, that is, there is a shared variance term across all $K$ classes. For simplicity, denote $\\sigma^2$.\n",
    "    \n",
    "&emsp;&emsp;$p_k(x) \n",
    "= \\dfrac{\\pi_k \\dfrac{1}{\\sqrt{2\\pi}\\sigma} \\exp{\\left(-\\dfrac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}}{\\sum\\limits_{l=1}^K{\\pi_l \\dfrac{1}{\\sqrt{2\\pi}\\sigma} \\exp{\\left(-\\dfrac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)}}} \\hspace{1cm}(*)$\n",
    "\n",
    "The Bayes classifier involves assigning an observation $X=x$ to the class for which (\\*) is largest.\n",
    "\n",
    "Taking the log of (\\*) and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which the following equation is largest.\n",
    "\n",
    "&emsp;&emsp;$\\delta_k(x) = x \\cdot \\dfrac{\\mu_k}{\\sigma^2} - \\dfrac{\\mu_k^2}{2\\sigma^2} + \\log{(\\pi_k)} \\hspace{1cm}$\n",
    "\n",
    "**Example**: $K=2$ and $\\pi_1 = \\pi_2$\n",
    "> Bayes classifier assigns an observation to class 1 if $2x(\\mu_1 - \\mu_2) > \\mu_1^2 - \\mu_2^2$ and to class 2 otherwise.\n",
    ">\n",
    "> Bayes decision boundary corresponds to the point where: $x = \\dfrac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1 - \\mu_2)} = \\dfrac{\\mu_1 + \\mu_2}{2}$\n",
    ">\n",
    "> <img src='images/4.4.PNG' width='550px'>\n",
    ">\n",
    "> The mean and variance parameters for the two density functions are: $\\mu_1 = -1.25$, $\\mu_2 = 1.25$, and $\\sigma_1^2 = \\sigma_2^2 = 1$. \n",
    "> \n",
    "> If we assume that an observation is equally likely to come from either class, i.e., $\\pi_1 = \\pi_2 = 0.5$, then Bayes classifier assigns observation to class 1 if $x<0$ and class 2 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we are not able to calculate the Bayes classifier since we still have to estimate $\\mu_1,...,\\mu_K$ and $\\sigma^2$.\n",
    "\n",
    "The LDA method approximates the Bayes classifier by plugging estimates for $\\pi_k$, $\\mu_k$, and $\\sigma^2$.\n",
    "\n",
    "&emsp;&emsp;\n",
    "$\\begin{cases}\n",
    "\\hat{\\mu}_k = \\dfrac{1}{n_k} \\sum\\limits_{i:y_i=k}{x_i} \\\\\n",
    "\\hat{\\sigma}^2 = \\dfrac{1}{n-K} \\sum\\limits_{k=1}^K \\sum\\limits_{i:y_i=k} {(x_i - \\hat{\\mu}_k)^2}\n",
    "\\end{cases} \\hspace{1cm}$ \n",
    "\n",
    "&emsp;where: $\\bullet\\hspace{0.2cm}$$n$: the total number of training observations <br>\n",
    "&emsp;$\\hspace{1.1cm}\\bullet\\hspace{0.2cm}$$n_k$: the number of training observations in the $k$ class\n",
    "\n",
    "LDA estimates $\\pi_k$ using the proportion of the training observations that belong to the $k$th class: $\\hat{\\pi}_k = \\dfrac{n_k}{n}$. <br>\n",
    "Plug $\\hat{\\mu}_k$, $\\hat{\\sigma}^2$, and $\\hat{\\pi}_k$ to $\\sigma_k(x)$ gives:\n",
    "$\\hat{\\sigma}_k(x) = x \\cdot \\dfrac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\dfrac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log{(\\hat{\\pi}_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA for $p>1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $X = (X_1,X_2,...,X_p)$ is drawn from a multivariate Gaussian distribution, with a class specific mean vector and a common covariance matrix.\n",
    "\n",
    "Assume that each individual predictor follows a 1D normal distribution with some correlation between each pair of predictors.\n",
    "\n",
    "<img src='images/4.5.PNG' width='500px'>\n",
    "\n",
    "* The height of the surface at any particular point represents the probability that both $X_1$ and $X_2$ fall in a small region around that point.\n",
    "* Cross-sections result from cutting the surface along $X_1$ or $X_2$ axis will have the shape of 1D normal distribution.\n",
    "* If $X_1$ and $X_2$ are non-correlated, then the base of the bell will be a circle. If correlated then that of the bell will have an elliptical shape.\n",
    "\n",
    "Denote $X \\sim N(\\mu, \\sum)$ as $p$-dimensional random variable $X$ has a multivariate Gaussian distribution.\n",
    "* $E(X) = \\mu$: the mean of $X$ (a vector with $p$ components)\n",
    "* $\\text{Cov}(X) = \\sum$: the $p \\times p$ covariance matrix of $X$\n",
    "\n",
    "**Multivariate Gaussian density**:\n",
    "$f(x) = \\dfrac{1}{(2\\pi)^{p/2} |\\sum|^{1/2}} \\exp{\\left(-\\dfrac{1}{2}(x-\\mu)^T \\sum^{-1}(x-\\mu)\\right)}$\n",
    "\n",
    "If more than 1 predictors:\n",
    "\n",
    "* LDA classifier assumes that the observations in the $k$th class are drawn from a multivariate Gaussian distribution $N(\\mu_k, \\sum)$ where $\\mu_k$ is a class-specific mean vector and $\\sum$ is a covariance matrix.\n",
    "* Plug $f_k(X=x)$ to Bayes' theorem $\\Rightarrow$ the Bayes classifier assigns an observation $X=x$ to the class for which the following equation is largest.\n",
    "\n",
    "    &emsp;&emsp;$\\delta_k(x) = x^T \\sum^{-1} \\mu_k - \\dfrac{1}{2} \\mu_k^T \\sum^{-1} \\mu_k + \\log{\\pi_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform LDA model fit to the 10000 training samples of Default data set results in a *training* error rate of 2.75\\%. <br>\n",
    "Two caveats:\n",
    "* Training error rates will usually be lower than test error rates.\n",
    "    * Because we adjust the paramerers of our model to do well on the training data.\n",
    "    * The higher the ratio of parameters $p$ to number of samples $n$, the more we expect this *overfitting* to play a role.\n",
    "* Since 3.33\\% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of credit card balance and student status, will result in an error rate of 3.33\\%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Confusion matrix shows LDA prediction on Default data set.\n",
    "<table class=\"tg\" style=\"float:right\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\" colspan=\"2\" rowspan=\"2\"></th>\n",
    "    <th class=\"tg-c3ow\" colspan=\"3\">True default status</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-baqh\">No</td>\n",
    "    <td class=\"tg-baqh\">Yes</td>\n",
    "    <td class=\"tg-baqh\">Total</td>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <th class=\"tg-baqh\" rowspan=\"3\">Predicted<br>default<br>status</th>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">9644</td>\n",
    "    <td class=\"tg-c3ow\">252</td>\n",
    "    <td class=\"tg-c3ow\">9896</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Yes</td>\n",
    "    <td class=\"tg-c3ow\">23</td>\n",
    "    <td class=\"tg-c3ow\">81</td>\n",
    "    <td class=\"tg-c3ow\">104</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Total</td>\n",
    "    <td class=\"tg-c3ow\">9667</td>\n",
    "    <td class=\"tg-c3ow\">333</td>\n",
    "    <td class=\"tg-c3ow\">10000</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<blockquote style='margin-right:250px'> \n",
    "    \n",
    "* LDA predicted that a total of 104 people would default.\n",
    "    * 81 actually defaulted\n",
    "    * 23 did not\n",
    "\n",
    "&emsp;$\\Rightarrow$ 23 out of 9667 of the individuals who did not default were incorrectly labeled.\n",
    "\n",
    "* Out of 333 individuals who defaulted, 252 were missed by LDA.\n",
    "\n",
    "$\\Rightarrow$ while the overall error rate is low, the error rate among individuals who defaulted is very high.\n",
    "\n",
    "From the perspective of a credit card company that is trying to identify high-risk individuals, an error rate of $252\\big/333 = 75.7\\%$ among individuals who default may well be unacceptable.\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why does it have such a low sensitivity?*\n",
    "\n",
    "* LDA is trying to approximate the Bayes classifier, which has the lowest **total** error rate out of all classifier (if the Gaussian model is correct), i.e., Bayes' classifier will yield the smallest possible total number of misclassified observations.\n",
    "\n",
    "A credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default is less problematic.\n",
    "\n",
    "* A fix for this particular interest is by changing the threshold from 0.5 to, maybe 0.2.\n",
    "\n",
    "    &emsp;&emsp;$\\Pr{(\\text{default} = \\text{Yes} | X=x)} > 0.2$\n",
    "    \n",
    "    $\\Rightarrow$ error rate decrease from $75.7\\%$ to $41.1\\%$; overall error increase to $3.73\\%$.\n",
    "    \n",
    "<img src='images/4.7.PNG' width='450px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How can we decide which threshold value is best?*\n",
    "\n",
    "* **ROC** (or **Receiver Operating Characteristics**) curve.\n",
    "    * The overall performance of a classifier, the summarized over all possible thresholds, is given by the area under the ROC curve (AUC).\n",
    "* An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. \n",
    "* ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds.\n",
    "\n",
    "For Default data, the AUC is 0.95, which is close to the maximum of one so would be considered very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\" colspan=\"2\" rowspan=\"2\"></th>\n",
    "    <th class=\"tg-c3ow\" colspan=\"3\" style=\"text-align:center\">Predicted class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">- or Null</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">+ or Not-null</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">Total</td>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\" rowspan=\"3\" style=\"text-align:left\">True class</th>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">- or Null</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">True Neg. (TN)</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">False Pos. (FP)</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:center\">N</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">+ or Non-null</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">False Neg. (FN)</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">True Pos. (TP)</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:center\">P</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">Total</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:center\">N*</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:center\">P*</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\"></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table class=\"tg\" style='text-align: left'>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\" style=\"text-align:left\">Name</th>\n",
    "    <th class=\"tg-0lax\" style=\"text-align:left\">Definition</th>\n",
    "    <th class=\"tg-0lax\" style=\"text-align:left\">Synonyms</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">False Pos. rate</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:center\">FP/N</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Type I error, 1-Specificity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">True Pos. rate</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:center\">TP/P</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Recall, 1-Type II error, power, sensitivity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Pos. Pred. value</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:center\">TP/P*</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Precision, 1-false discovery proportion</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Neg. Pred. value</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:center\">TN/N*</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\"></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic discriminant analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QDA classifier assumes that the observations from each class drawn from a Gaussian distribution and has its own covariance matrix. That is, $X \\sim N(\\mu, \\sum_k)$.\n",
    "\n",
    "The Bayes classifier assigns an observation $X=x$ to the class for which the following equation is largest.\n",
    "\n",
    "&emsp;&emsp;$\\begin{split}\n",
    "\\sigma_k(x)\n",
    "&= \\textstyle -\\dfrac{1}{2} (x - \\mu_k)^T \\sum_k^{-1} (x - \\mu_k) - \\dfrac{1}{2} \\log(|\\sum_k|) + \\log(\\pi_k) \\\\\n",
    "&= \\textstyle -\\dfrac{1}{2} x^T \\sum_k^{-1} x + x^T \\sum_k^{-1} \\mu_k - \\dfrac{1}{2} \\mu_k^T \\sum_k^{-1} \\mu_k - \\dfrac{1}{2} \\log(|\\sum_k|) + \\log(\\pi_k)\n",
    "\\end{split}$\n",
    "\n",
    "*Why does it matter whether or not we assume that the $K$ classes share a common covariance matrix?*\n",
    "\n",
    "* When there are $p$ predictors, then estimating a covariance matrix requires estimating $p(p+1)\\big/2$ parameters. $\\Rightarrow$ a lot of parameter\n",
    "* If LDA’s assumption that the $K$ classes share a common covariance matrix is badly off, then LDA can suffer from high bias.\n",
    "* LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. \n",
    "* QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the $K$ classes is clearly untenable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A comparison of classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider 2-class setting with $p=1$ predictor, and let $p_1(x)$ and $p_2(x) = 1 - p_1(x)$ be the probabilities that the observation $X=x$ belongs to class 1 and class 2, respectively.\n",
    "\n",
    "* Log odds of LDA: $\\log{\\left(\\dfrac{p_1(x)}{1 - p_1(x)}\\right)} = \\log{\\left(\\dfrac{p_1(x)}{p_2(x)}\\right)} = c_0 + c_1x$\n",
    "\n",
    "    where $c_0$ and $c_1$ are functions of $\\mu_1$, $\\mu_2$, and $\\sigma$. <br><br>\n",
    "    \n",
    "* Log odds of logistic regression: $\\log{\\left(\\dfrac{p_1}{1-p_1}\\right)} = \\beta_0 + \\beta_1x$\n",
    "\n",
    "Same: Both are linear functions of $x$.\n",
    "\n",
    "Different: $\\beta_0$ and $\\beta_1$ are estimated using maximum likelihood, while $c_0$ and $c_1$ mean and variance from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NON-LINEAR METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN: \n",
    "* To make a prediction for an observation $X=x$, the $K$ training observations that are closest to $x$ are identified. Then $X$ is assigned to the class to which the plurality of these observations belong.\n",
    "* No assumptions are made about the shape of the decision boundary.\n",
    "* Does tell use which predictors are important.\n",
    "\n",
    "QDA:\n",
    "* Assume a quadratic decision boundary.\n",
    "* Peform better in the present of a limited number of training observations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCENARIOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $p=2$ predictors.\n",
    "\n",
    "<img src='images/4.10.PNG' width='750px'>\n",
    "\n",
    "* **Scenario 1**: 20 training observations in each class. The observations within each class were uncorrelated random normal variables with a different mean in each class.\n",
    "\n",
    "* **Scenario 2**: Same as scenario 1, except that within each class, the two predictors has a correlation of -0.5.\n",
    "\n",
    "* **Scenario 3**: $X_1$ and $X_2$ are generated from the $t$-distribution with 50 observations per class. The $t$-distribution has a similar shape to the normal distribution but it has a tendency to yield more extreme points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/4.11.PNG' width='750px'>\n",
    "\n",
    "* **Scenario 4**: The data were generated from a normal distribution with a correlation of 0.5 between the predictors in the first class, and correlation of -0.5 between the predictors in the second class.\n",
    "\n",
    "* **Scenario 5**: The data were generated from a normal distribution with uncorrelated predictors. However, the responses were sampled from the logistic function using $X_1^2$, $X_2^2$, and $X_1 \\times X_2$ as predictors.\n",
    "\n",
    "* **Scenario 6**: Same as scenario 5, but the responses were sampled from a more complicated non-linear functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
