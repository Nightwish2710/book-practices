{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".CodeMirror pre {\n",
       "font-size: 10pt;\n",
       "}\n",
       "\n",
       "div.code_cell {\n",
       "font-size: 10pt;\n",
       "}\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:0em;\n",
       "margin-bottom:0em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size:1.7em;\n",
       "line-height:1em;\n",
       "text-align:left;\n",
       "margin-top:-0.2em;\n",
       "margin-bottom:-0.2em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "font-size:1.4em;\n",
       "margin-top:-0.2em;\n",
       "margin-bottom:-0.2em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h3 { /*  Parts names nearer from text */\n",
       "font-size:1.1em;\n",
       "margin-top:-0.2em;\n",
       "margin-bottom:-0.2em;\n",
       "}\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-size:9.5pt;\n",
       "line-height:130%;\n",
       "margin-top:-0.2em;\n",
       "margin-bottom:-0.2em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import HessianInversionWarning\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore', HessianInversionWarning)\n",
    "\n",
    "def compute_model(y_name, X_name, data):\n",
    "    y = data.loc[:, y_name]\n",
    "    X = sm.add_constant(data.loc[:, X_name].values)\n",
    "    model = sm.Logit(y, X).fit(disp=0)\n",
    "    \n",
    "    return show_table(model, X_name)\n",
    "    \n",
    "def show_table(model, X_name):\n",
    "    index_name = ['Intercept']\n",
    "    if isinstance(X_name, str):\n",
    "        index_name.append(X_name)\n",
    "    elif isinstance(X_name, list):\n",
    "        index_name = index_name + X_name\n",
    "    \n",
    "    df = pd.read_html(model.summary2().as_html())[1]\n",
    "    colname = df.iloc[0]\n",
    "    df = df.rename(columns=df.iloc[0]).drop(0).set_index(np.nan)\n",
    "    df.index.name = None\n",
    "    df.index = index_name\n",
    "    \n",
    "    return df\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>\n",
    "\n",
    ".CodeMirror pre {\n",
    "font-size: 10pt;\n",
    "}\n",
    "\n",
    "div.code_cell {\n",
    "font-size: 10pt;\n",
    "}\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:0em;\n",
    "margin-bottom:0em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size:1.7em;\n",
    "line-height:1em;\n",
    "text-align:left;\n",
    "margin-top:-0.2em;\n",
    "margin-bottom:-0.2em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "font-size:1.4em;\n",
    "margin-top:-0.2em;\n",
    "margin-bottom:-0.2em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h3 { /*  Parts names nearer from text */\n",
    "font-size:1.1em;\n",
    "margin-top:-0.2em;\n",
    "margin-bottom:-0.2em;\n",
    "}\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-size:9.5pt;\n",
    "line-height:130%;\n",
    "margin-top:-0.2em;\n",
    "margin-bottom:-0.2em;\n",
    "}\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = pd.read_csv('data/Default.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An overview of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training observations: $(x_1,y_1), (x_2,y_2), ..., (x_n,y_n)$.\n",
    "\n",
    "Use the Default data set to illustrate the concept of classification.\n",
    "* Build a model to predict default ($Y$) for any given value of balance ($X_1$) and income ($X_2$).\n",
    "\n",
    "In most real application, the relationship between the predictor and the response will not be nearly so strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why not linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are trying to predict the medical condition of a patient in the emergency room. There are 3 possible diagnoses: stroke, drug overdose, and epileptic seizure. We could encode these variables as:\n",
    "\n",
    "&emsp;&emsp;$Y = \\begin{cases}\n",
    "1 &\\text{ if stroke} \\\\\n",
    "2 &\\text{ if drug overdose} \\\\\n",
    "3 &\\text{ if epileptic seizure}\n",
    "\\end{cases}$\n",
    "\n",
    "However, this coding implies an ordering on the outcomes.\n",
    "\n",
    "If the response variable's values did take on a natural ordering, such as *mild*, *moderate*, and *severe*, then a 1, 2, 3 coding would be reasonable.\n",
    "\n",
    "In general, there is no natural way to convert a qualitative response variable with more than 2 levels into a quantitative response that is ready for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Default data, logistic regression models the probability of default.\n",
    "\n",
    "* E.g., the probability of default given balance: $\\Pr(\\text{default} = \\text{Yes} | \\text{balance})$\n",
    "    * Abbreviation: $p(\\text{balance})$\n",
    "    * Range: between 0 and 1\n",
    "    * If $p(\\text{balance})$ higher than a certain threshold, then $\\text{default} = \\text{Yes}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How should we model the relationship between $p(X) = \\Pr(Y=1|X)$ and $X$?*\n",
    "\n",
    "If we use $p(X) = \\beta_0 + \\beta_1X$ to predict $\\text{default} = \\text{Yes}$ using $\\text{balance}$, then:\n",
    "* for balances close to zero, we predict a negative probability of default\n",
    "* for very large balances, we would get values bigger than 1.\n",
    "\n",
    "$\\Rightarrow$ not sensible because probability must fall between 0 and 1.\n",
    "\n",
    "To avoid this, we must model $p(x)$ using a function that gives output between 0 and 1 for all values of $X$. \n",
    "\n",
    "**Logistic function**: $p(X) = \\dfrac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}$.\n",
    "\n",
    "* This function will always produce and S-shaped curve.\n",
    "* For low balances, predict probability close to zero.\n",
    "* For high balance, predict probability close to one.\n",
    "\n",
    "**Odd**: $\\dfrac{p(X)}{1-p(X)}$ (take any value between 0 and $\\infty$). \n",
    "\n",
    "**Log-odds** (or **logit**): $\\log\\left(\\dfrac{p(X)}{1-p(X)}\\right)$ (here $\\log$ has base $e$).\n",
    "\n",
    "* Increase $X$ by one unit changes the log odds by $\\beta_1$.\n",
    "* Because relationship between $p(X)$ and $X$ is not a straight line, $\\beta_1$ doesn't correspond to the change in $p(X)$ associated with a one-unit increase in $X$.\n",
    "    * The amount that $p(X)$ changes due to a one-unit change in $X$ will depend on the current value of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the regression coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum likelihood**: seek estimates for $\\beta_0$ and $\\beta_1$ such that the predicted probability $\\hat{p}(x_i)$ of default for each individual corresponds as closely as possible to the individual's observed default status.\n",
    "\n",
    "* find $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that plugging these estimated into the model for $p(X)$ yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not.\n",
    "\n",
    "**Likelihood function**: $l(\\beta_0,\\beta_1) = \\prod\\limits_{i:y_i=1}{p(x_i)}\\prod\\limits_{i':y_i'=0}{(1-p(x'_i))}$\n",
    "\n",
    "* The estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are chosen to **maximize** this likelihood function.\n",
    "* The least square approach in linear regression is a special case of maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef.</th>\n",
       "      <th>Std.Err.</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "      <th>[0.025</th>\n",
       "      <th>0.975]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Intercept</td>\n",
       "      <td>-10.6513</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>-29.4913</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-11.3592</td>\n",
       "      <td>-9.9435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>balance</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>24.9524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Coef. Std.Err.         z   P>|z|    [0.025   0.975]\n",
       "Intercept  -10.6513   0.3612  -29.4913  0.0000  -11.3592  -9.9435\n",
       "balance      0.0055   0.0002   24.9524  0.0000    0.0051   0.0059"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table 4.1\n",
    "default_dummy = pd.get_dummies(default, columns=['default'])\n",
    "compute_model('default_Yes', 'balance', default_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Table 4.1.\n",
    "> The table shows the coefficient estimated and related information that result from fitting a logistic regression model on the Default data to predict $\\Pr(\\text{default} = \\text{Yes} | \\text{balance})$.\n",
    ">\n",
    "> A one-unit increase in balance is associated with an increase in the log odds of default by 0.0055 units.\n",
    ">\n",
    "> Null hypothesis: $p(X) = \\dfrac{e^{\\beta_0}}{1 + e^{\\beta_0}}$. <br>\n",
    "> $\\Rightarrow$ large (absolute) value of $z$-statistic indicates evidence against null hypothesis. <br>\n",
    "> $\\Rightarrow$ $p$-value associated with balance is tiny $\\rightarrow$ reject $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the coefficients have been estimated, compute the probability of default given any credit card balance.\n",
    "\n",
    "**Example**: $\\text{balance} = \\$1000$.\n",
    "> $\\hat{p}(X) \n",
    "= \\dfrac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}\n",
    "= \\dfrac{e^{-10.6513 + 0.0055 \\times 1000}}{1 + e^{-10.6513 + 0.0055 \\times 1000}}\n",
    "= 0.00575$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef.</th>\n",
       "      <th>Std.Err.</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "      <th>[0.025</th>\n",
       "      <th>0.975]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Intercept</td>\n",
       "      <td>-3.5041</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>-49.5541</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-3.6427</td>\n",
       "      <td>-3.3655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>student_Yes</td>\n",
       "      <td>0.4049</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>3.5202</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.1795</td>\n",
       "      <td>0.6303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Coef. Std.Err.         z   P>|z|   [0.025   0.975]\n",
       "Intercept    -3.5041   0.0707  -49.5541  0.0000  -3.6427  -3.3655\n",
       "student_Yes   0.4049   0.1150    3.5202  0.0004   0.1795   0.6303"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table 4.2\n",
    "default_dummy = pd.get_dummies(default, columns=['default', 'student'])\n",
    "compute_model('default_Yes', 'student_Yes', default_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Table 4.2.\n",
    "> $\\hat{\\Pr}(\\text{default} = \\text{Yes} | \\text{Student} = \\text{Yes}) \n",
    "= \\dfrac{e^{-3.5041 + 0.4049 \\times 1}}{1 + e^{-3.5041 + 0.4049 \\times 1}} = 0.0431$\n",
    ">\n",
    "> $\\hat{\\Pr}(\\text{default} = \\text{Yes} | \\text{Student} = \\text{No}) \n",
    "= \\dfrac{e^{-3.5041 + 0.4049 \\times 0}}{1 + e^{-3.5041 + 0.4049 \\times 0}} = 0.0292$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analogy with the extension from simple to multiple linear regression, we can generalize the formula as follow:\n",
    "\n",
    "&emsp;&emsp;$\\log{\\left(\\dfrac{p(X)}{1 - p(X)}\\right)} \n",
    "= \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$\n",
    "\n",
    "&emsp;&emsp;$p(X) = \\dfrac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef.</th>\n",
       "      <th>Std.Err.</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "      <th>[0.025</th>\n",
       "      <th>0.975]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Intercept</td>\n",
       "      <td>-10.8690</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>-22.0793</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-11.8339</td>\n",
       "      <td>-9.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>balance</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>24.7365</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>income</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3698</td>\n",
       "      <td>0.7115</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>student_Yes</td>\n",
       "      <td>-0.6468</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>-2.7376</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>-1.1098</td>\n",
       "      <td>-0.1837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Coef. Std.Err.         z   P>|z|    [0.025   0.975]\n",
       "Intercept    -10.8690   0.4923  -22.0793  0.0000  -11.8339  -9.9042\n",
       "balance        0.0057   0.0002   24.7365  0.0000    0.0053   0.0062\n",
       "income         0.0000   0.0000    0.3698  0.7115   -0.0000   0.0000\n",
       "student_Yes   -0.6468   0.2363   -2.7376  0.0062   -1.1098  -0.1837"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table 4.3\n",
    "default_dummy = pd.get_dummies(default, columns=['default', 'student'])\n",
    "compute_model('default_Yes', ['balance', 'income', 'student_Yes'], default_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Table 4.3.\n",
    "> $p$-value associated with balance and student are very small. <br>\n",
    "> $\\Rightarrow$ these variables is associated with the probability of default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Given a credit card balance of $\\$1500$ and an income $\\$40000$.\n",
    "> A student has: $\\hat{p}(X)\n",
    "= \\dfrac{e^{-10.869 + 0.00574 \\times 1500 + 0.003 \\times 40000 - 0.6468 \\times 1}}{1 + e^{-10.869 + 0.00574 \\times 1500 + 0.003 \\times 40000 - 0.6468 \\times 1}}\n",
    "= 0.058$\n",
    ">\n",
    "> A non-student has: $\\hat{p}(X)\n",
    "= \\dfrac{e^{-10.869 + 0.00574 \\times 1500 + 0.003 \\times 40000 - 0.6468 \\times 0}}{1 + e^{-10.869 + 0.00574 \\times 1500 + 0.003 \\times 40000 - 0.6468 \\times 0}}\n",
    "= 0.105$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Notice that the coefficient of student in table 4.2 is positive while that of student in table 4.3 is negative. *How is this possible?*\n",
    "\n",
    "> <img src='images/4.3.PNG' width='570px'>\n",
    ">\n",
    "> The negative coefficient for student in the multiple logistic regression indicates that for a **fixed value** of balance and income, a student is less likely to default than a non-student.\n",
    ">\n",
    "> The horizontal broken lines near the base of the plot, which show the default rates for students and non-students average over all values of balance and income, suggest: the overall student default rate is higher than the non-student default rate. $\\Rightarrow$ there is a positive coefficient for student in the single variable logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables student and balance are correlated.\n",
    "\n",
    "* Students tend to hold higher levels of debt, which is in turn associated with higher probability of default and more likely to have large credit card balances. \n",
    "* Thus, even though an individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance, the fact that students on the whole tend to have higher credit card balances means that overall, students tend to default at a higher rate than non-students.\n",
    "\n",
    "$\\Rightarrow$ a student is riskier than a non-student if no information about the student's credit card balance is available. However, that student is **less risky** than a non-student **with the same credit card balance**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border:1px solid black; padding:10px'>\n",
    "\n",
    "**Confounder** (also **confounding variable**, **confounding factor**, or **lurking variable**): a variable that influences both the dependent variable and independent variable, causing a spurious association.\n",
    "\n",
    "**Spurious relationship** (or **spurious correlation**): a mathematical relationship in which two or more events or variables are associated but not causally related, due to either coincidence or the presence of a certain third, unseen factor.\n",
    "\n",
    "**Example**: Two events can cause grass to be wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkle (when it rains, the sprinkler is usually not active).\n",
    "> <img src='images/bayesian_network.png' width='430px'>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear discriminant analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression involves directly modeling $\\Pr{(Y=k | X=x)}$ using the logistic function. \n",
    "\n",
    "Consider an alternative approach where we model the distribution of $X$ separately in each of the response classes, and then use Bayes' theorem to flip these around into estimates for $\\Pr{(Y=k | X=x)}$.\n",
    "\n",
    "Why do we need another method?\n",
    "\n",
    "* When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. LDA is not.\n",
    "* If $n$ is small and the distribution of the predictors $X$ is approximately normal in each of the classes, LDA is more statble than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bayes' theorem for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='float:right; border:1px solid black; padding:10px; padding-left:15px; width:260px'>\n",
    "\n",
    "**Posterior probability**: the probability an event will happen after all evidence or background information has been taken into account. It is closely related to prior probability, which is the probability an event will happen before you taken any new evidence into account.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style='margin-right:280px'>\n",
    "Suppose that we classify an observation into one of $K$ classes ($K \\geq 2$).\n",
    "\n",
    "* Let $p_k(X) = \\Pr{(Y=k | X)}$ be the posterior probability that an observation $X = x$ belongs to the $k$th class.\n",
    "* Let $\\pi_k$ represent the overall or prior probability that a randomly chosen observation is associated with the $k$th category of the response variable $Y$.\n",
    "* Let $f_k(x) \\equiv \\Pr{(X=x | Y=k)}$ denote the density function of $X$ for an observation that comes from the $k$th class.\n",
    "    * $f_k(x)$ large $\\Rightarrow$ very likely that an observation in the $k$th class has $X \\approx x$.\n",
    "    * $f_k(x)$ small $\\Rightarrow$ very unlikely that an observation in the $k$th class has $X \\approx x$.\n",
    "</div>\n",
    "\n",
    "**Bayes' theorem**: \n",
    "$\\Pr{(Y=k | X=x)} = \\dfrac{\\pi_k f_k(x)}{\\sum\\limits_{l=1}^K{\\pi_l f_l(x)}}$\n",
    "\n",
    "* This equation suggests that instead of directly computing $p_k(X)$, we can plug in estimates for $\\pi_k$ and $f_k(X)$.\n",
    "    * Estimating $\\pi_k$ is easy if we have a random sample of $Y$s from the population.\n",
    "        * Compute the fraction of the training observations that belong to the $k$th class.\n",
    "    * Estimating $f_k(X)$ is more challenging, unless we assume some simple forms for these densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA for $p=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we only have one predictor. Hence, $p=1$. We want to obtain an estimate for $f_k(x)$ to plug into Bayes' theorem to estimate $p_k(x)$.\n",
    "\n",
    "Suppose we assume that $f_k(x)$ is *normal* or *Gaussian*.\n",
    "\n",
    "Normal density: $f_k(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp{\\left(-\\dfrac{1}{2\\sigma_k^2}(x - \\mu_k)^2\\right)}$\n",
    "    \n",
    "&emsp;where $\\mu_k$ and $\\sigma_k^2$ are the mean and variance parameters for the $k$th class.\n",
    "    \n",
    "Assume that $\\sigma_1^2 = ... = \\sigma_K^2$, that is, there is a shared variance term across all $K$ classes. For simplicity, denote $\\sigma^2$.\n",
    "    \n",
    "&emsp;&emsp;$p_k(x) \n",
    "= \\dfrac{\\pi_k \\dfrac{1}{\\sqrt{2\\pi}\\sigma} \\exp{\\left(-\\dfrac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}}{\\sum\\limits_{l=1}^K{\\pi_l \\dfrac{1}{\\sqrt{2\\pi}\\sigma} \\exp{\\left(-\\dfrac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)}}} \\hspace{1cm}(*)$\n",
    "\n",
    "The Bayes classifier involves assigning an observation $X=x$ to the class for which (\\*) is largest.\n",
    "\n",
    "Taking the log of (\\*) and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which the following equation is largest.\n",
    "\n",
    "&emsp;&emsp;$\\delta_k(x) = x \\cdot \\dfrac{\\mu_k}{\\sigma^2} - \\dfrac{\\mu_k^2}{2\\sigma^2} + \\log{(\\pi_k)} \\hspace{1cm}$\n",
    "\n",
    "**Example**: $K=2$ and $\\pi_1 = \\pi_2$\n",
    "> Bayes classifier assigns an observation to class 1 if $2x(\\mu_1 - \\mu_2) > \\mu_1^2 - \\mu_2^2$ and to class 2 otherwise.\n",
    ">\n",
    "> Bayes decision boundary corresponds to the point where: $x = \\dfrac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1 - \\mu_2)} = \\dfrac{\\mu_1 + \\mu_2}{2}$\n",
    ">\n",
    "> <img src='images/4.4.PNG' width='550px'>\n",
    ">\n",
    "> The mean and variance parameters for the two density functions are: $\\mu_1 = -1.25$, $\\mu_2 = 1.25$, and $\\sigma_1^2 = \\sigma_2^2 = 1$. \n",
    "> \n",
    "> If we assume that an observation is equally likely to come from either class, i.e., $\\pi_1 = \\pi_2 = 0.5$, then Bayes classifier assigns observation to class 1 if $x<0$ and class 2 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we are not able to calculate the Bayes classifier since we still have to estimate $\\mu_1,...,\\mu_K$ and $\\sigma^2$.\n",
    "\n",
    "The LDA method approximates the Bayes classifier by plugging estimates for $\\pi_k$, $\\mu_k$, and $\\sigma^2$.\n",
    "\n",
    "&emsp;&emsp;\n",
    "$\\begin{cases}\n",
    "\\hat{\\mu}_k = \\dfrac{1}{n_k} \\sum\\limits_{i:y_i=k}{x_i} \\\\\n",
    "\\hat{\\sigma}^2 = \\dfrac{1}{n-K} \\sum\\limits_{k=1}^K \\sum\\limits_{i:y_i=k} {(x_i - \\hat{\\mu}_k)^2}\n",
    "\\end{cases} \\hspace{1cm}$ \n",
    "\n",
    "&emsp;where: $\\bullet\\hspace{0.2cm}$$n$: the total number of training observations <br>\n",
    "&emsp;$\\hspace{1.1cm}\\bullet\\hspace{0.2cm}$$n_k$: the number of training observations in the $k$ class\n",
    "\n",
    "LDA estimates $\\pi_k$ using the proportion of the training observations that belong to the $k$th class: $\\hat{\\pi}_k = \\dfrac{n_k}{n}$. <br>\n",
    "Plug $\\hat{\\mu}_k$, $\\hat{\\sigma}^2$, and $\\hat{\\pi}_k$ to $\\sigma_k(x)$ gives:\n",
    "$\\hat{\\sigma}_k(x) = x \\cdot \\dfrac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\dfrac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log{(\\hat{\\pi}_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA for $p>1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $X = (X_1,X_2,...,X_p)$ is drawn from a multivariate Gaussian distribution, with a class specific mean vector and a common covariance matrix.\n",
    "\n",
    "Assume that each individual predictor follows a 1D normal distribution with some correlation between each pair of predictors.\n",
    "\n",
    "<img src='images/4.5.PNG' width='500px'>\n",
    "\n",
    "* The height of the surface at any particular point represents the probability that both $X_1$ and $X_2$ fall in a small region around that point.\n",
    "* Cross-sections result from cutting the surface along $X_1$ or $X_2$ axis will have the shape of 1D normal distribution.\n",
    "* If $X_1$ and $X_2$ are non-correlated, then the base of the bell will be a circle. If correlated then that of the bell will have an elliptical shape.\n",
    "\n",
    "Denote $X \\sim N(\\mu, \\sum)$ as $p$-dimensional random variable $X$ has a multivariate Gaussian distribution.\n",
    "* $E(X) = \\mu$: the mean of $X$ (a vector with $p$ components)\n",
    "* $\\text{Cov}(X) = \\sum$: the $p \\times p$ covariance matrix of $X$\n",
    "\n",
    "**Multivariate Gaussian density**:\n",
    "$f(x) = \\dfrac{1}{(2\\pi)^{p/2} |\\sum|^{1/2}} \\exp{\\left(-\\dfrac{1}{2}(x-\\mu)^T \\sum^{-1}(x-\\mu)\\right)}$\n",
    "\n",
    "If more than 1 predictors:\n",
    "\n",
    "* LDA classifier assumes that the observations in the $k$th class are drawn from a multivariate Gaussian distribution $N(\\mu_k, \\sum)$ where $\\mu_k$ is a class-specific mean vector and $\\sum$ is a covariance matrix.\n",
    "* Plug $f_k(X=x)$ to Bayes' theorem $\\Rightarrow$ the Bayes classifier assigns an observation $X=x$ to the class for which the following equation is largest.\n",
    "\n",
    "    &emsp;&emsp;$\\delta_k(x) = x^T \\sum^{-1} \\mu_k - \\dfrac{1}{2} \\mu_k^T \\sum^{-1} \\mu_k + \\log{\\pi_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform LDA model fit to the 10000 training samples of Default data set results in a *training* error rate of 2.75\\%. <br>\n",
    "Two caveats:\n",
    "* Training error rates will usually be lower than test error rates.\n",
    "    * Because we adjust the paramerers of our model to do well on the training data.\n",
    "    * The higher the ratio of parameters $p$ to number of samples $n$, the more we expect this *overfitting* to play a role.\n",
    "* Since 3.33\\% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of credit card balance and student status, will result in an error rate of 3.33\\%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Confusion matrix shows LDA prediction on Default data set.\n",
    "<table class=\"tg\" style=\"float:right\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\" colspan=\"2\" rowspan=\"2\"></th>\n",
    "    <th class=\"tg-c3ow\" colspan=\"3\">True default status</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-baqh\">No</td>\n",
    "    <td class=\"tg-baqh\">Yes</td>\n",
    "    <td class=\"tg-baqh\">Total</td>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <th class=\"tg-baqh\" rowspan=\"3\">Predicted<br>default<br>status</th>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">9644</td>\n",
    "    <td class=\"tg-c3ow\">252</td>\n",
    "    <td class=\"tg-c3ow\">9896</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Yes</td>\n",
    "    <td class=\"tg-c3ow\">23</td>\n",
    "    <td class=\"tg-c3ow\">81</td>\n",
    "    <td class=\"tg-c3ow\">104</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Total</td>\n",
    "    <td class=\"tg-c3ow\">9667</td>\n",
    "    <td class=\"tg-c3ow\">333</td>\n",
    "    <td class=\"tg-c3ow\">10000</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<blockquote style='margin-right:250px'> \n",
    "    \n",
    "* LDA predicted that a total of 104 people would default.\n",
    "    * 81 actually defaulted\n",
    "    * 23 did not\n",
    "\n",
    "&emsp;$\\Rightarrow$ 23 out of 9667 of the individuals who did not default were incorrectly labeled.\n",
    "\n",
    "* Out of 333 individuals who defaulted, 252 were missed by LDA.\n",
    "\n",
    "$\\Rightarrow$ while the overall error rate is low, the error rate among individuals who defaulted is very high.\n",
    "\n",
    "From the perspective of a credit card company that is trying to identify high-risk individuals, an error rate of $252\\big/333 = 75.7\\%$ among individuals who default may well be unacceptable.\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why does it have such a low sensitivity?*\n",
    "\n",
    "* LDA is trying to approximate the Bayes classifier, which has the lowest **total** error rate out of all classifier (if the Gaussian model is correct), i.e., Bayes' classifier will yield the smallest possible total number of misclassified observations.\n",
    "\n",
    "A credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default is less problematic.\n",
    "\n",
    "* A fix for this particular interest is by changing the threshold from 0.5 to, maybe 0.2.\n",
    "\n",
    "    &emsp;&emsp;$\\Pr{(\\text{default} = \\text{Yes} | X=x)} > 0.2$\n",
    "    \n",
    "    $\\Rightarrow$ error rate decrease from $75.7\\%$ to $41.1\\%$; overall error increase to $3.73\\%$.\n",
    "    \n",
    "<img src='images/4.7.PNG' width='450px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How can we decide which threshold value is best?*\n",
    "\n",
    "* **ROC** (or **Receiver Operating Characteristics**) curve.\n",
    "    * The overall performance of a classifier, the summarized over all possible thresholds, is given by the area under the ROC curve (AUC).\n",
    "* An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. \n",
    "* ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds.\n",
    "\n",
    "For Default data, the AUC is 0.95, which is close to the maximum of one so would be considered very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\" colspan=\"2\" rowspan=\"2\"></th>\n",
    "    <th class=\"tg-c3ow\" colspan=\"3\" style=\"text-align:center\">Predicted class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">- or Null</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">+ or Not-null</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">Total</td>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\" rowspan=\"3\" style=\"text-align:left\">True class</th>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">- or Null</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">True Neg. (TN)</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">False Pos. (FP)</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:center\">N</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">+ or Non-null</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">False Neg. (FN)</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">True Pos. (TP)</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:center\">P</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\">Total</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:center\">N*</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:center\">P*</td>\n",
    "    <td class=\"tg-c3ow\" style=\"text-align:left\"></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table class=\"tg\" style='text-align: left'>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\" style=\"text-align:left\">Name</th>\n",
    "    <th class=\"tg-0lax\" style=\"text-align:left\">Definition</th>\n",
    "    <th class=\"tg-0lax\" style=\"text-align:left\">Synonyms</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">False Pos. rate</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:center\">FP/N</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Type I error, 1-Specificity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">True Pos. rate</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:center\">TP/P</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Recall, 1-Type II error, power, sensitivity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Pos. Pred. value</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:center\">TP/P*</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Precision, 1-false discovery proportion</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\">Neg. Pred. value</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:center\">TN/N*</td>\n",
    "    <td class=\"tg-0lax\" style=\"text-align:left\"></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic discriminant analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QDA classifier assumes that the observations from each class drawn from a Gaussian distribution and has its own covariance matrix. That is, $X \\sim N(\\mu, \\sum_k)$.\n",
    "\n",
    "The Bayes classifier assigns an observation $X=x$ to the class for which the following equation is largest.\n",
    "\n",
    "&emsp;&emsp;$\\begin{split}\n",
    "\\sigma_k(x)\n",
    "&= \\textstyle -\\dfrac{1}{2} (x - \\mu_k)^T \\sum_k^{-1} (x - \\mu_k) - \\dfrac{1}{2} \\log(|\\sum_k|) + \\log(\\pi_k) \\\\\n",
    "&= \\textstyle -\\dfrac{1}{2} x^T \\sum_k^{-1} x + x^T \\sum_k^{-1} \\mu_k - \\dfrac{1}{2} \\mu_k^T \\sum_k^{-1} \\mu_k - \\dfrac{1}{2} \\log(|\\sum_k|) + \\log(\\pi_k)\n",
    "\\end{split}$\n",
    "\n",
    "*Why does it matter whether or not we assume that the $K$ classes share a common covariance matrix?*\n",
    "\n",
    "* When there are $p$ predictors, then estimating a covariance matrix requires estimating $p(p+1)\\big/2$ parameters. $\\Rightarrow$ a lot of parameter\n",
    "* If LDA’s assumption that the $K$ classes share a common covariance matrix is badly off, then LDA can suffer from high bias.\n",
    "* LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. \n",
    "* QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the $K$ classes is clearly untenable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A comparison of classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider 2-class setting with $p=1$ predictor, and let $p_1(x)$ and $p_2(x) = 1 - p_1(x)$ be the probabilities that the observation $X=x$ belongs to class 1 and class 2, respectively.\n",
    "\n",
    "* Log odds of LDA: $\\log{\\left(\\dfrac{p_1(x)}{1 - p_1(x)}\\right)} = \\log{\\left(\\dfrac{p_1(x)}{p_2(x)}\\right)} = c_0 + c_1x$\n",
    "\n",
    "    where $c_0$ and $c_1$ are functions of $\\mu_1$, $\\mu_2$, and $\\sigma$. <br><br>\n",
    "    \n",
    "* Log odds of logistic regression: $\\log{\\left(\\dfrac{p_1}{1-p_1}\\right)} = \\beta_0 + \\beta_1x$\n",
    "\n",
    "Same: Both are linear functions of $x$.\n",
    "\n",
    "Different: $\\beta_0$ and $\\beta_1$ are estimated using maximum likelihood, while $c_0$ and $c_1$ mean and variance from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NON-LINEAR METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN: \n",
    "* To make a prediction for an observation $X=x$, the $K$ training observations that are closest to $x$ are identified. Then $X$ is assigned to the class to which the plurality of these observations belong.\n",
    "* No assumptions are made about the shape of the decision boundary.\n",
    "* Does tell use which predictors are important.\n",
    "\n",
    "QDA:\n",
    "* Assume a quadratic decision boundary.\n",
    "* Peform better in the present of a limited number of training observations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCENARIOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $p=2$ predictors.\n",
    "\n",
    "<img src='images/4.10.PNG' width='750px'>\n",
    "\n",
    "* **Scenario 1**: 20 training observations in each class. The observations within each class were uncorrelated random normal variables with a different mean in each class.\n",
    "\n",
    "* **Scenario 2**: Same as scenario 1, except that within each class, the two predictors has a correlation of -0.5.\n",
    "\n",
    "* **Scenario 3**: $X_1$ and $X_2$ are generated from the $t$-distribution with 50 observations per class. The $t$-distribution has a similar shape to the normal distribution but it has a tendency to yield more extreme points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/4.11.PNG' width='750px'>\n",
    "\n",
    "* **Scenario 4**: The data were generated from a normal distribution with a correlation of 0.5 between the predictors in the first class, and correlation of -0.5 between the predictors in the second class.\n",
    "\n",
    "* **Scenario 5**: The data were generated from a normal distribution with uncorrelated predictors. However, the responses were sampled from the logistic function using $X_1^2$, $X_2^2$, and $X_1 \\times X_2$ as predictors.\n",
    "\n",
    "* **Scenario 6**: Same as scenario 5, but the responses were sampled from a more complicated non-linear functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
